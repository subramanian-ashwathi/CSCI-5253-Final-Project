{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b386379d-2a25-433e-8175-ad8ba9fbecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to display all the matplotlib graphs inside the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Hiding the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e516c4d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, expr, lpad, min, max, substring, hour, concat_ws, to_timestamp, regexp_replace, trim, initcap, datediff, date_trunc, month, year\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbc2109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://data.lacity.org/resource/2nrs-mtv8.json?$select=count(*)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Convert data to a Spark DataFrame\u001b[39;00m\n\u001b[0;32m     10\u001b[0m crime_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
    "\n",
    "# Download JSON data\n",
    "url = 'https://data.lacity.org/resource/2nrs-mtv8.json?$select=count(*)'\n",
    "response = requests.get(url)\n",
    "data = json.loads(response.text)\n",
    "\n",
    "# Convert data to a Spark DataFrame\n",
    "crime_count = int(data[0]['count'])\n",
    "n_crimes = crime_count\n",
    "\n",
    "print('The number of crimes till date is: ', n_crimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560fac23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").config(\"spark.jars\", \"hdfs://maskxdc/test/flint-0.6.0.jar\").getOrCreate()\n",
    "\n",
    "# Define API URL\n",
    "API_URL = \"https://data.lacity.org/resource/2nrs-mtv8.json\"\n",
    "\n",
    "def get_count(start_date, end_date):\n",
    "    query = f\"{API_URL}?$select=count(*)&$where=date_rptd between '{start_date}' and '{end_date}'\"\n",
    "    res = requests.get(query)\n",
    "    if res.status_code != 200:\n",
    "        raise Exception(f\"Return code: {res.status_code}\\tReturn Text: {res.content}\")\n",
    "\n",
    "    records_count = int(res.json()[0]['count'])\n",
    "    return records_count\n",
    "\n",
    "def get_data(start_date, end_date, limit, batch_num):\n",
    "    err_corr = int(limit * 0.1)\n",
    "\n",
    "    offset = (batch_num * limit) - err_corr if batch_num != 0 else 0\n",
    "    limit = limit + 2 * err_corr\n",
    "    query = (\n",
    "        f\"{API_URL}?$order=date_rptd ASC&$where=date_rptd between '{start_date}' and '{end_date}'&$limit={limit}&$offset={offset}\"\n",
    "    )\n",
    "    res = requests.get(query)\n",
    "    if res.status_code != 200:\n",
    "        raise Exception(f\"Return code: {res.status_code}\\tReturn Text: {res.content}\")\n",
    "\n",
    "    data = res.json()\n",
    "    return spark.createDataFrame(data)\n",
    "\n",
    "def load_data(start_date, end_date, batch_size=50000, debug=True):\n",
    "    if debug:\n",
    "        # Assuming you have a CSV file in the same format as the original code\n",
    "        return spark.read.csv(\"la_crime_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Get total count of columns to load\n",
    "    records_count = get_count(start_date, end_date)\n",
    "    print(f\"Fetching {records_count} records...\")\n",
    "\n",
    "    num_batches = int(records_count / batch_size) + 1\n",
    "\n",
    "    # Fetch data in batches\n",
    "    all_data = []\n",
    "    for batch_num in range(num_batches):\n",
    "        batch_data = get_data(start_date, end_date, batch_size, batch_num)\n",
    "        all_data.append(batch_data)\n",
    "\n",
    "    return all_data[0].union(*all_data[1:]).dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2087400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 852950\n",
      "Number of columns: 28\n"
     ]
    }
   ],
   "source": [
    "start_date = '2020-01-01'\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# crime_data = load_data(start_date, end_date, debug=False)\n",
    "# crime_data.to_csv('la_crime_data.csv', index=False)\n",
    "crime_data = load_data(start_date, end_date)\n",
    "\n",
    "# Display the shape of the DataFrame\n",
    "num_rows = crime_data.count()\n",
    "num_columns = len(crime_data.columns)\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c207e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another DataFrame 'data' and store the content of 'crime_data'\n",
    "data = crime_data.alias(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10270d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Crimes data DataFrame is: (852950, 28)\n"
     ]
    }
   ],
   "source": [
    "def get_shape(df):\n",
    "    num_rows = df.count()\n",
    "    num_columns = len(df.columns)\n",
    "    return num_rows, num_columns\n",
    "\n",
    "# Usage\n",
    "num_rows, num_columns = get_shape(data)\n",
    "print(\"Shape of the Crimes data DataFrame is:\", (num_rows, num_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8e485a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+--------+----+-----------+-----------+--------+------+--------------------+-------------------+--------+--------+------------+---------+--------------------+------+-----------+--------+--------+--------------------+-------+---------+--------------+--------------------+--------------------+--------+--------+\n",
      "|    dr_no|          date_rptd|           date_occ|time_occ|area|  area_name|rpt_dist_no|part_1_2|crm_cd|         crm_cd_desc|            mocodes|vict_age|vict_sex|vict_descent|premis_cd|         premis_desc|status|status_desc|crm_cd_1|crm_cd_2|            location|    lat|      lon|weapon_used_cd|         weapon_desc|        cross_street|crm_cd_3|crm_cd_4|\n",
      "+---------+-------------------+-------------------+--------+----+-----------+-----------+--------+------+--------------------+-------------------+--------+--------+------------+---------+--------------------+------+-----------+--------+--------+--------------------+-------+---------+--------------+--------------------+--------------------+--------+--------+\n",
      "|191501505|2020-01-01 00:00:00|2020-01-01 00:00:00|    1730|  15|N Hollywood|       1543|       2|   745|VANDALISM - MISDE...|          0329 1402|      76|       F|           W|      502|MULTI-UNIT DWELLI...|    IC|Invest Cont|     745|     998|5400    CORTEEN  ...|34.1685|-118.4019|          NULL|                NULL|                NULL|    NULL|    NULL|\n",
      "|191921269|2020-01-01 00:00:00|2020-01-01 00:00:00|     415|  19|    Mission|       1998|       2|   740|VANDALISM - FELON...|               0329|      31|       X|           X|      409| BEAUTY SUPPLY STORE|    IC|Invest Cont|     740|    NULL|14400    TITUS   ...|34.2198|-118.4468|          NULL|                NULL|                NULL|    NULL|    NULL|\n",
      "|200104020|2020-01-01 00:00:00|2020-01-01 00:00:00|     245|   1|    Central|        153|       1|   350|       THEFT, PERSON|0344 0416 0446 1822|      44|       M|           B|      102|            SIDEWALK|    IC|Invest Cont|     350|     624|6TH              ...|34.0463|-118.2515|           400|STRONG-ARM (HANDS...|SPRING           ...|    NULL|    NULL|\n",
      "|200104024|2020-01-01 00:00:00|2020-01-01 00:00:00|     325|   1|    Central|        134|       2|   740|VANDALISM - FELON...|               0329|      41|       M|           A|      122|VEHICLE, PASSENGE...|    IC|Invest Cont|     740|    NULL|4TH              ...|34.0503|-118.2504|          NULL|                NULL|HILL             ...|    NULL|    NULL|\n",
      "|200104027|2020-01-01 00:00:00|2020-01-01 00:00:00|      30|   1|    Central|        182|       2|   930|CRIMINAL THREATS ...|     0416 0421 1822|      57|       M|           O|      108|         PARKING LOT|    IC|Invest Cont|     930|    NULL|700 W  9TH       ...|34.0458|-118.2614|           400|STRONG-ARM (HANDS...|                NULL|    NULL|    NULL|\n",
      "+---------+-------------------+-------------------+--------+----+-----------+-----------+--------+------+--------------------+-------------------+--------+--------+------------+---------+--------------------+------+-----------+--------+--------+--------------------+-------+---------+--------------+--------------------+--------------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows of the DataFrame\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9dfcebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape before removing duplicates from the dataset is: (852950, 28)\n",
      "The shape after removing duplicates from the dataset is: (852950, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape before removing duplicates from the dataset is:\", (num_rows, num_columns))\n",
    "\n",
    "# Remove duplicates\n",
    "data = data.dropDuplicates()\n",
    "\n",
    "# Check the shape after removing duplicates\n",
    "num_rows_after = data.count()\n",
    "print(\"The shape after removing duplicates from the dataset is:\", (num_rows_after, num_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f427d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dr_no: integer (nullable = true)\n",
      " |-- date_rptd: timestamp (nullable = true)\n",
      " |-- date_occ: timestamp (nullable = true)\n",
      " |-- time_occ: integer (nullable = true)\n",
      " |-- area: integer (nullable = true)\n",
      " |-- area_name: string (nullable = true)\n",
      " |-- rpt_dist_no: integer (nullable = true)\n",
      " |-- part_1_2: integer (nullable = true)\n",
      " |-- crm_cd: integer (nullable = true)\n",
      " |-- crm_cd_desc: string (nullable = true)\n",
      " |-- mocodes: string (nullable = true)\n",
      " |-- vict_age: integer (nullable = true)\n",
      " |-- vict_sex: string (nullable = true)\n",
      " |-- vict_descent: string (nullable = true)\n",
      " |-- premis_cd: integer (nullable = true)\n",
      " |-- premis_desc: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- status_desc: string (nullable = true)\n",
      " |-- crm_cd_1: integer (nullable = true)\n",
      " |-- crm_cd_2: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- weapon_used_cd: integer (nullable = true)\n",
      " |-- weapon_desc: string (nullable = true)\n",
      " |-- cross_street: string (nullable = true)\n",
      " |-- crm_cd_3: integer (nullable = true)\n",
      " |-- crm_cd_4: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrame\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54acf4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------+--------+----+---------+-----------+--------+------+-----------+------------------+--------+------------------+------------------+--------------------+--------------------+------+-----------+--------------------+-----------------+--------+---+---+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|dr_no|date_rptd|date_occ|time_occ|area|area_name|rpt_dist_no|part_1_2|crm_cd|crm_cd_desc|           mocodes|vict_age|          vict_sex|      vict_descent|           premis_cd|         premis_desc|status|status_desc|            crm_cd_1|         crm_cd_2|location|lat|lon|   weapon_used_cd|      weapon_desc|     cross_street|         crm_cd_3|         crm_cd_4|\n",
      "+-----+---------+--------+--------+----+---------+-----------+--------+------+-----------+------------------+--------+------------------+------------------+--------------------+--------------------+------+-----------+--------------------+-----------------+--------+---+---+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  0.0|      0.0|     0.0|     0.0| 0.0|      0.0|        0.0|     0.0|   0.0|        0.0|13.870801336537896|     0.0|13.201946186763585|13.202884108095434|0.001172401664810...|0.060730406237176855|   0.0|        0.0|0.001289641831291...|92.67002755143913|     0.0|0.0|0.0|65.20921507708542|65.20921507708542|84.09508177501613|99.75227152822556|99.99273110967818|\n",
      "+-----+---------+--------+--------+----+---------+-----------+--------+------+-----------+------------------+--------+------------------+------------------+--------------------+--------------------+------+-----------+--------------------+-----------------+--------+---+---+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of null values in each column\n",
    "num_rows = data.count()\n",
    "null_percentages = [(count(when(col(c).isNull(), c)) / num_rows * 100).alias(c) for c in data.columns]\n",
    "\n",
    "# Create a DataFrame with the null percentages\n",
    "null_percentage_df = data.agg(*null_percentages)\n",
    "\n",
    "# Show the null percentages\n",
    "null_percentage_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cb337fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+--------+----+---------+-----------+--------+------+--------------------+--------------------+--------+--------+------------+---------+-------------+------+-----------+--------+--------+--------------------+-------+---------+--------------+--------------------+------------+--------+--------+\n",
      "|    dr_no|          date_rptd|           date_occ|time_occ|area|area_name|rpt_dist_no|part_1_2|crm_cd|         crm_cd_desc|             mocodes|vict_age|vict_sex|vict_descent|premis_cd|  premis_desc|status|status_desc|crm_cd_1|crm_cd_2|            location|    lat|      lon|weapon_used_cd|         weapon_desc|cross_street|crm_cd_3|crm_cd_4|\n",
      "+---------+-------------------+-------------------+--------+----+---------+-----------+--------+------+--------------------+--------------------+--------+--------+------------+---------+-------------+------+-----------+--------+--------+--------------------+-------+---------+--------------+--------------------+------------+--------+--------+\n",
      "|200613424|2020-08-02 00:00:00|2020-08-02 00:00:00|    2030|   6|Hollywood|        657|       1|   761|     BRANDISH WEAPON|1402 2000 1813 18...|      34|       F|           H|      108|  PARKING LOT|    AO|Adult Other|     761|     920|             WESTERN|34.0885|-118.3092|           212|              BOTTLE|     ROMAINE|     930|     998|\n",
      "|210209196|2021-05-08 00:00:00|2021-05-08 00:00:00|     230|   2|  Rampart|        279|       1|   210|             ROBBERY|1817 0906 0913 03...|      43|       M|           H|      101|       STREET|    AO|Adult Other|     210|     510|        JAMES M WOOD|34.0503| -118.272|           500|UNKNOWN WEAPON/OT...|       GREEN|     910|     998|\n",
      "|210617136|2021-10-08 00:00:00|2021-10-07 00:00:00|    1950|   6|Hollywood|        659|       1|   121|      RAPE, FORCIBLE|1822 0342 0400 04...|      59|       F|           H|      710|OTHER PREMISE|    IC|Invest Cont|     121|     210|           NORMANDIE|34.0966|-118.3005|           205|       KITCHEN KNIFE|  DE LONGPRE|     910|     998|\n",
      "|221401314|2022-11-10 00:00:00|2022-11-10 00:00:00|    2117|  14|  Pacific|       1452|       2|   910|          KIDNAPPING|1259 2002 0503 05...|      15|       M|           W|      117|        BEACH|    IC|Invest Cont|     812|     860|          WASHINGTON|33.9792|-118.4666|           400|STRONG-ARM (HANDS...|    SPEEDWAY|     910|     998|\n",
      "|210105053|2021-01-25 00:00:00|2021-01-23 00:00:00|       1|   1|  Central|        131|       1|   420|THEFT FROM MOTOR ...|           0344 1822|      25|       M|           H|      108|  PARKING LOT|    IC|Invest Cont|     420|    NULL|100 S  FIGUEROA  ...|34.0569| -118.254|          NULL|                NULL|        NULL|    NULL|    NULL|\n",
      "+---------+-------------------+-------------------+--------+----+---------+-----------+--------+------+--------------------+--------------------+--------+--------+------------+---------+-------------+------+-----------+--------+--------+--------------------+-------+---------+--------------+--------------------+------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert 'date_rptd' and 'date_occ' columns to datetime\n",
    "data = data.withColumn(\"date_rptd\", col(\"date_rptd\").cast(\"timestamp\"))\n",
    "data = data.withColumn(\"date_occ\", col(\"date_occ\").cast(\"timestamp\"))\n",
    "\n",
    "# Show the DataFrame after conversion\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c15e3e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time_occ' column to string\n",
    "data = data.withColumn(\"time_occ\", col(\"time_occ\").cast(\"string\"))\n",
    "\n",
    "# Add leading zeros to make it four digits\n",
    "data = data.withColumn(\"time_occ\", lpad(col(\"time_occ\"), 4, '0'))\n",
    "\n",
    "# Format 'time_occ' as HH:mm\n",
    "data = data.withColumn(\"time_occ\", concat_ws(\":\", substring(col(\"time_occ\"), 1, 2), substring(col(\"time_occ\"), 3, 2)))\n",
    "\n",
    "# Cast 'time_occ' to timestamp\n",
    "data = data.withColumn(\"time_occ\", to_timestamp(col(\"time_occ\"), \"HH:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e76a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age groups and labels\n",
    "age_groups = [\n",
    "    (col(\"vict_age\") < 1),\n",
    "    (col(\"vict_age\").between(1, 12)),\n",
    "    (col(\"vict_age\").between(13, 17)),\n",
    "    (col(\"vict_age\").between(18, 64)),\n",
    "    (col(\"vict_age\") >= 65)\n",
    "]\n",
    "\n",
    "labels = ['Unknown', 'Child', 'Teenager', 'Adult', 'Old']\n",
    "\n",
    "# Create new column 'Age Group'\n",
    "data = data.withColumn(\"Age Group\", when(age_groups[0], labels[0])\n",
    "                                   .when(age_groups[1], labels[1])\n",
    "                                   .when(age_groups[2], labels[2])\n",
    "                                   .when(age_groups[3], labels[3])\n",
    "                                   .when(age_groups[4], labels[4])\n",
    "                                   .otherwise(\"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cedd1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in 'vict_sex' column\n",
    "data = data.withColumn(\"vict_sex\", when((col(\"vict_sex\") == 'M'), 'Male')\n",
    "                                   .when((col(\"vict_sex\") == 'F'), 'Female')\n",
    "                                   .when((col(\"vict_sex\") == 'X') | (col(\"vict_sex\") == 'H') | (col(\"vict_sex\").isNull()), 'Unknown')\n",
    "                                   .otherwise(col(\"vict_sex\")))\n",
    "\n",
    "# Fill null values with 'Data Missing'\n",
    "data = data.na.fill('Data Missing', subset=[\"vict_sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dd700bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|vict_descent|count |\n",
      "+------------+------+\n",
      "|K           |4579  |\n",
      "|F           |3580  |\n",
      "|NULL        |112614|\n",
      "|B           |120896|\n",
      "|L           |56    |\n",
      "|V           |893   |\n",
      "|U           |170   |\n",
      "|O           |67532 |\n",
      "|D           |66    |\n",
      "|C           |3313  |\n",
      "|J           |1181  |\n",
      "|-           |2     |\n",
      "|Z           |426   |\n",
      "|A           |18700 |\n",
      "|X           |83214 |\n",
      "|W           |173440|\n",
      "|S           |46    |\n",
      "|G           |63    |\n",
      "|I           |805   |\n",
      "|P           |229   |\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get unique values and their counts in 'vict_descent' column\n",
    "unique_values_counts = data.groupBy(\"vict_descent\").count()\n",
    "\n",
    "# Show the result\n",
    "unique_values_counts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d40e9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define descent mapping\n",
    "descent_mapping = {\n",
    "    'A': 'Other Asian',\n",
    "    'B': 'Black',\n",
    "    'C': 'Chinese',\n",
    "    'D': 'Cambodian',\n",
    "    'F': 'Filipino',\n",
    "    'G': 'Guamanian',\n",
    "    'H': 'Hispanic/Latin/Mexican',\n",
    "    'I': 'American Indian/Alaskan Native',\n",
    "    'J': 'Japanese',\n",
    "    'K': 'Korean',\n",
    "    'L': 'Laotian',\n",
    "    'O': 'Other',\n",
    "    'P': 'Pacific Islander',\n",
    "    'S': 'Samoan',\n",
    "    'U': 'Hawaiian',\n",
    "    'V': 'Vietnamese',\n",
    "    'W': 'White',\n",
    "    'X': 'Unknown',\n",
    "    '-': 'Unknown',\n",
    "    'Z': 'Asian Indian'\n",
    "}\n",
    "\n",
    "# Replace values in 'vict_descent' column\n",
    "for key, value in descent_mapping.items():\n",
    "    data = data.withColumn(\"vict_descent\", when((col(\"vict_descent\") == key), value).otherwise(col(\"vict_descent\")))\n",
    "\n",
    "# Fill null values with 'Data Missing'\n",
    "data = data.na.fill(\"Data Missing\", subset=[\"vict_descent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae4c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------+\n",
      "|vict_descent                  |count |\n",
      "+------------------------------+------+\n",
      "|Asian Indian                  |426   |\n",
      "|American Indian/Alaskan Native|805   |\n",
      "|Chinese                       |3313  |\n",
      "|Data Missing                  |112614|\n",
      "|Hawaiian                      |170   |\n",
      "|Japanese                      |1181  |\n",
      "|Hispanic/Latin/Mexican        |261145|\n",
      "|Unknown                       |83216 |\n",
      "|Filipino                      |3580  |\n",
      "|Vietnamese                    |893   |\n",
      "|Other                         |67532 |\n",
      "|Pacific Islander              |229   |\n",
      "|Samoan                        |46    |\n",
      "|Guamanian                     |63    |\n",
      "|Korean                        |4579  |\n",
      "|Laotian                       |56    |\n",
      "|White                         |173440|\n",
      "|Cambodian                     |66    |\n",
      "|Black                         |120896|\n",
      "|Other Asian                   |18700 |\n",
      "+------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get unique values and their counts in 'vict_descent' column\n",
    "unique_values_counts = data.groupBy(\"vict_descent\").count()\n",
    "\n",
    "# Show the result\n",
    "unique_values_counts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c643d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in 'premis_desc' column with 'Data Missing'\n",
    "data = data.na.fill(\"Data Missing\", subset=[\"premis_desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39c974e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|status_desc |count |\n",
      "+------------+------+\n",
      "|Juv Arrest  |2775  |\n",
      "|UNK         |4     |\n",
      "|Juv Other   |1484  |\n",
      "|Invest Cont |683107|\n",
      "|Adult Other |91576 |\n",
      "|Adult Arrest|74004 |\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get unique values and their counts in 'status_desc' column\n",
    "unique_values_counts = data.groupBy(\"status_desc\").count()\n",
    "\n",
    "# Show the result\n",
    "unique_values_counts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d5b6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in 'weapon_desc' column with 'Data Missing'\n",
    "data = data.na.fill(\"Data Missing\", subset=[\"weapon_desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32a58a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'cross_street' and 'mocodes' columns with 'Data Missing'\n",
    "data = data.na.fill(\"Data Missing\", subset=[\"cross_street\", \"mocodes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3421711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dr_no',\n",
       " 'Date Reported',\n",
       " 'Date Occurred',\n",
       " 'Time Occurred',\n",
       " 'Area Code',\n",
       " 'area_name',\n",
       " 'rpt_dist_no',\n",
       " 'Part 1-2',\n",
       " 'Crime Code',\n",
       " 'Crime Description',\n",
       " 'Modus Operandi',\n",
       " 'Victim Age',\n",
       " 'Victim Gender',\n",
       " 'Victim Descent',\n",
       " 'premis_cd',\n",
       " 'Crime Location',\n",
       " 'status',\n",
       " 'Case Status',\n",
       " 'crm_cd_1',\n",
       " 'crm_cd_2',\n",
       " 'Street Address',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'weapon_used_cd',\n",
       " 'Weapon Description',\n",
       " 'cross_street',\n",
       " 'crm_cd_3',\n",
       " 'crm_cd_4',\n",
       " 'Age Group']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns based on the provided mapping\n",
    "data = data \\\n",
    "    .withColumnRenamed(\"lat\", \"Latitude\") \\\n",
    "    .withColumnRenamed(\"lon\", \"Longitude\") \\\n",
    "    .withColumnRenamed(\"status_desc\", \"Case Status\") \\\n",
    "    .withColumnRenamed(\"premis_desc\", \"Crime Location\") \\\n",
    "    .withColumnRenamed(\"vict_descent\", \"Victim Descent\") \\\n",
    "    .withColumnRenamed(\"vict_sex\", \"Victim Gender\") \\\n",
    "    .withColumnRenamed(\"weapon_desc\", \"Weapon Description\") \\\n",
    "    .withColumnRenamed(\"vict_age\", \"Victim Age\") \\\n",
    "    .withColumnRenamed(\"mocodes\", \"Modus Operandi\") \\\n",
    "    .withColumnRenamed(\"date_rptd\", \"Date Reported\") \\\n",
    "    .withColumnRenamed(\"date_occ\", \"Date Occurred\") \\\n",
    "    .withColumnRenamed(\"time_occ\", \"Time Occurred\") \\\n",
    "    .withColumnRenamed(\"part_1_2\", \"Part 1-2\") \\\n",
    "    .withColumnRenamed(\"location\", \"Street Address\") \\\n",
    "    .withColumnRenamed(\"crm_cd\", \"Crime Code\") \\\n",
    "    .withColumnRenamed(\"crm_cd_desc\", \"Crime Description\") \\\n",
    "    .withColumnRenamed(\"area\", \"Area Code\")\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa7088a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['premis_cd', 'crm_cd_1', 'crm_cd_2', 'crm_cd_3', 'crm_cd_4', 'status', 'weapon_used_cd']\n",
    "\n",
    "# Select columns that are not in the list of columns to drop\n",
    "data = data.select([column for column in data.columns if column not in columns_to_drop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fcf0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with None in 'Latitude' and 'Longitude' columns because\n",
    "# 0s represent location unknow, which are masked for privacy with 0s\n",
    "# this step is done to avoid any confusion in map plots in Tableau\n",
    "\n",
    "data = data.withColumn(\"Latitude\", when(col(\"Latitude\") == 0, None).otherwise(col(\"Latitude\")))\n",
    "data = data.withColumn(\"Longitude\", when(col(\"Longitude\") == 0, None).otherwise(col(\"Longitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91ac8d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr No',\n",
       " 'Date Reported',\n",
       " 'Date Occurred',\n",
       " 'Time Occurred',\n",
       " 'Area Code',\n",
       " 'Area Name',\n",
       " 'Rpt Dist No',\n",
       " 'Part 1-2',\n",
       " 'Crime Code',\n",
       " 'Crime Description',\n",
       " 'Modus Operandi',\n",
       " 'Victim Age',\n",
       " 'Victim Gender',\n",
       " 'Victim Descent',\n",
       " 'Crime Location',\n",
       " 'Case Status',\n",
       " 'Street Address',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Weapon Description',\n",
       " 'Cross Street',\n",
       " 'Age Group']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace underscores with spaces and title-case column names\n",
    "for column in data.columns:\n",
    "    new_column = column.replace(\"_\", \" \").title()\n",
    "    data = data.withColumnRenamed(column, new_column)\n",
    "    \n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "657a0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove text inside brackets\n",
    "def remove_text_inside_brackets(sentence):\n",
    "    return trim(regexp_replace(sentence, '\\(.*?\\)', ''))\n",
    "\n",
    "# Apply the function to each row in the 'Crime Description' column\n",
    "data = data.withColumn('Crime Description', remove_text_inside_brackets(col('Crime Description')))\n",
    "data = data.withColumn('Crime Description', regexp_replace(col('Crime Description'), ',', '-'))\n",
    "data = data.withColumn('Crime Description', initcap(trim(col('Crime Description'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "110042fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the 'Weapon Description' column\n",
    "data = data.withColumn('Weapon Description', remove_text_inside_brackets(col('Weapon Description')))\n",
    "data = data.withColumn('Weapon Description', regexp_replace(col('Weapon Description'), ',', '-'))\n",
    "data = data.withColumn('Weapon Description', initcap(trim(col('Weapon Description'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36980f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to the 'Crime Location' column\n",
    "data = data.withColumn('Crime Location', trim(col('Crime Location')))\n",
    "data = data.withColumn('Crime Location', initcap(col('Crime Location')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7247644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace asterisks in the 'Crime Location' column\n",
    "data = data.withColumn('Crime Location', regexp_replace(col('Crime Location'), '\\\\*', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec333076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to the 'Street Address' column\n",
    "data = data.withColumn('Street Address', trim(col('Street Address')))\n",
    "data = data.withColumn('Street Address', initcap(col('Street Address')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a3a0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove extra spaces\n",
    "def remove_extra_spaces(address_string):\n",
    "    return regexp_replace(address_string, ' +', ' ')\n",
    "\n",
    "# Apply the function to the 'Street Address' column\n",
    "data = data.withColumn('Street Address', remove_extra_spaces(col('Street Address')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afd06256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-------------+---------+---------+-----------+--------+----------+-----------------+--------------+----------+-------------+--------------+--------------+-----------+--------------+-------------------+-------------------+------------------+------------+---------+\n",
      "|Dr No|Date Reported|Date Occurred|Time Occurred|Area Code|Area Name|Rpt Dist No|Part 1-2|Crime Code|Crime Description|Modus Operandi|Victim Age|Victim Gender|Victim Descent|Crime Location|Case Status|Street Address|           Latitude|          Longitude|Weapon Description|Cross Street|Age Group|\n",
      "+-----+-------------+-------------+-------------+---------+---------+-----------+--------+----------+-----------------+--------------+----------+-------------+--------------+--------------+-----------+--------------+-------------------+-------------------+------------------+------------+---------+\n",
      "|  0.0|          0.0|          0.0|          0.0|      0.0|      0.0|        0.0|     0.0|       0.0|              0.0|           0.0|       0.0|          0.0|           0.0|           0.0|        0.0|           0.0|0.26531449674658536|0.26531449674658536|               0.0|         0.0|      0.0|\n",
      "+-----+-------------+-------------+-------------+---------+---------+-----------+--------+----------+-----------------+--------------+----------+-------------+--------------+--------------+-----------+--------------+-------------------+-------------------+------------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of null values in each columnafter data cleaning\n",
    "num_rows = data.count()\n",
    "null_percentages = [(count(when(col(c).isNull(), c)) / num_rows * 100).alias(c) for c in data.columns]\n",
    "\n",
    "# Create a DataFrame with the null percentages\n",
    "null_percentage_df = data.agg(*null_percentages)\n",
    "\n",
    "# Show the null percentages\n",
    "null_percentage_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bea9713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 852950 crimes committed over 1433 days. On average, there are 595 crimes each day.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the duration\n",
    "\n",
    "min_date = data.select(min('Date Occurred')).first()[0]\n",
    "max_date = data.select(max('Date Occurred')).first()[0]\n",
    "duration = (max_date - min_date).days\n",
    "\n",
    "print(\"There are {} crimes committed over {} days. On average, there are {} crimes each day.\".format(data.count(), duration, int(data.count() / duration)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bd99343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the delay\n",
    "data = data.withColumn('Delay', datediff(col('Date Reported'), col('Date Occurred')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02c17626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels\n",
    "bins = [-1, 0, 1, 2, 7, 30, 365, float('inf')]\n",
    "labels = [\"Same day\", \"1 day\", \"2 days\", '3-7 days', '8-30 days', '1 month-1 year', 'More than 1 year']\n",
    "\n",
    "# Create 'Delay Category' column\n",
    "data = data.withColumn('Delay Category',\n",
    "    when((col('Delay') >= bins[0]) & (col('Delay') < bins[1]), labels[0])\n",
    "    .when((col('Delay') >= bins[1]) & (col('Delay') < bins[2]), labels[1])\n",
    "    .when((col('Delay') >= bins[2]) & (col('Delay') < bins[3]), labels[2])\n",
    "    .when((col('Delay') >= bins[3]) & (col('Delay') < bins[4]), labels[3])\n",
    "    .when((col('Delay') >= bins[4]) & (col('Delay') < bins[5]), labels[4])\n",
    "    .when((col('Delay') >= bins[5]) & (col('Delay') < bins[6]), labels[5])\n",
    "    .otherwise(labels[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b50c6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Week Period' column\n",
    "data = data.withColumn('Week Period', date_trunc('week', col('Date Occurred')))\n",
    "\n",
    "# Create 'Month Period' column\n",
    "data = data.withColumn('Month Period', date_trunc('month', col('Date Occurred')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "069d0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Time Slot' column\n",
    "data = data.withColumn(\n",
    "    'Time Slot',\n",
    "    when((col('Time Occurred').cast('long') >= float('-inf')) & (col('Time Occurred').cast('long') < 1.59 * 3600), '0-2')\n",
    "    .when((col('Time Occurred').cast('long') >= 1.59 * 3600) & (col('Time Occurred').cast('long') < 3.59 * 3600), '2-4')\n",
    "    .when((col('Time Occurred').cast('long') >= 3.59 * 3600) & (col('Time Occurred').cast('long') < 5.59 * 3600), '4-6')\n",
    "    .when((col('Time Occurred').cast('long') >= 5.59 * 3600) & (col('Time Occurred').cast('long') < 7.59 * 3600), '6-8')\n",
    "    .when((col('Time Occurred').cast('long') >= 7.59 * 3600) & (col('Time Occurred').cast('long') < 9.59 * 3600), '8-10')\n",
    "    .when((col('Time Occurred').cast('long') >= 9.59 * 3600) & (col('Time Occurred').cast('long') < 11.59 * 3600), '10-12')\n",
    "    .when((col('Time Occurred').cast('long') >= 11.59 * 3600) & (col('Time Occurred').cast('long') < 13.59 * 3600), '12-14')\n",
    "    .when((col('Time Occurred').cast('long') >= 13.59 * 3600) & (col('Time Occurred').cast('long') < 15.59 * 3600), '14-16')\n",
    "    .when((col('Time Occurred').cast('long') >= 15.59 * 3600) & (col('Time Occurred').cast('long') < 17.59 * 3600), '16-18')\n",
    "    .when((col('Time Occurred').cast('long') >= 17.59 * 3600) & (col('Time Occurred').cast('long') < 19.59 * 3600), '18-20')\n",
    "    .when((col('Time Occurred').cast('long') >= 19.59 * 3600) & (col('Time Occurred').cast('long') < 21.59 * 3600), '20-22')\n",
    "    .when((col('Time Occurred').cast('long') >= 21.59 * 3600) & (col('Time Occurred').cast('long') < float('inf')), '22-24')\n",
    "    .otherwise(None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "012e7f46-81b1-46ac-9f70-b70f482cb900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "432b7e40-1c1f-489a-adcf-4a14dd478fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o912.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\spark\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\spark\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:607)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save DataFrame to CSV\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessed-los-angeles-data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o912.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\spark\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\spark\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:607)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n"
     ]
    }
   ],
   "source": [
    "# Save DataFrame to CSV\n",
    "data.write.mode(\"overwrite\").csv(r\"processed-los-angeles-data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee3321e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1e905-9fcc-4642-ae86-161028c42f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "940b61ea",
   "metadata": {},
   "source": [
    "## Time Series Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf874b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series_data = data.select('Date Occurred')\n",
    "\n",
    "# time_series_data = data.groupBy('Date Occurred').count().withColumnRenamed('count', 'Crime Count')\n",
    "\n",
    "# Show the resulting time series data\n",
    "# time_series_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e14ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series_data = time_series_data.withColumn('Month', month('Date Occurred'))\n",
    "# time_series_data = time_series_data.withColumn('Year', year('Date Occurred'))\n",
    "\n",
    "# Show the resulting time series data\n",
    "# time_series_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff975ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Year' and 'Month' and pivot the data\n",
    "# crosstab_data = time_series_data.groupBy('Year').pivot('Month').agg(F.sum('Crime Count'))\n",
    "\n",
    "# Show the resulting crosstab data\n",
    "# crosstab_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698af476-93ce-4ce4-a965-fdfac3f1c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_los_angeles_crime_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e9dcd-ed7a-45a0-8b65-a4026ce48267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_series_data = data[['Date Occured']][:]\n",
    "time_series_data = time_series_data.groupby(['Date Occured']).size().reset_index(name='Crime Count')\n",
    "# Convert the string to a datetime object\n",
    "time_series_data['Date Occured'] = pd.to_datetime(time_series_data['Date Occured'])\n",
    "time_series_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e0688-ad1c-4727-91aa-b843a9e9540e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to get month from a date\n",
    "def Function_get_month(inpDate):\n",
    "    return(inpDate.month)\n",
    "\n",
    "# Function to get Year from a date\n",
    "def Function_get_year(inpDate):\n",
    "    return(inpDate.year)\n",
    "\n",
    "# Creating new columns\n",
    "time_series_data['Month']=time_series_data['Date Occured'].apply(Function_get_month)\n",
    "time_series_data['Year']=time_series_data['Date Occured'].apply(Function_get_year)\n",
    "\n",
    "time_series_data.head()\n",
    "time_series_data.set_index('Date Occured', inplace=True)\n",
    "time_series_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548bb82-56ae-45e2-b0ae-242ed5e317ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the sales quantity for each month for all categories\n",
    "pd.crosstab(columns=time_series_data['Month'],\n",
    "            index=time_series_data['Year'],\n",
    "            values=time_series_data['Crime Count'],\n",
    "            aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c879e21-2ef4-4e26-884d-b20490fd5760",
   "metadata": {},
   "outputs": [],
   "source": [
    "CrimeCount=pd.crosstab(columns=time_series_data['Year'],\n",
    "            index=time_series_data['Month'],\n",
    "            values=time_series_data['Crime Count'],\n",
    "            aggfunc='sum').melt()['value']\n",
    "\n",
    "MonthNames=['Jan','Feb','Mar','Apr','May', 'Jun', 'Jul', 'Aug', 'Sep','Oct','Nov','Dec']*4\n",
    "\n",
    "# Plotting the sales\n",
    "%matplotlib inline\n",
    "CrimeCount.plot(kind='line', figsize=(16,5), title='Total Crime Count per Month')\n",
    "# Setting the x-axis labels\n",
    "plotLabels=plt.xticks(np.arange(0,48,1),MonthNames, rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d88e90f-ebec-429b-b34b-d23b77db917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = CrimeCount.values\n",
    "result = seasonal_decompose(series, model='additive', period=12)\n",
    "result.plot()\n",
    "CurrentFig=plt.gcf()\n",
    "CurrentFig.set_size_inches(11,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29062f-5fa8-4e8c-9da5-1d6b47ce7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "CrimeCount = CrimeCount[:len(CrimeCount)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bdd67-4f20-41e5-8ff0-7b3369ffa3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CrimeCount = (CrimeCount/3800000)*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810f53d-42be-4661-be70-a477fa7af887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the full dataset \n",
    "SarimaxModel = model = SARIMAX(CrimeCount,  \n",
    "                        order = (5, 1, 10),  \n",
    "                        seasonal_order = (6, 0, 0, 12))\n",
    "CrimeModel = SarimaxModel.fit()\n",
    "  \n",
    "# Forecast for the next 6 months\n",
    "forecast = CrimeModel.predict(start = 0,\n",
    "                          end = (len(CrimeCount)+7)-1,\n",
    "                          typ = 'levels').rename('Forecast')\n",
    "print(\"Next Six Month Forecast:\",forecast[-7:])\n",
    "\n",
    "# Plot the forecast values\n",
    "CrimeCount.plot(figsize = (10, 15), legend = True, title='Time Series Crime Count Forecasts')\n",
    "forecast.plot(legend = True, figsize=(18,5))\n",
    "\n",
    "# Measuring the accuracy of the model\n",
    "MAPE=np.mean(abs(CrimeCount-forecast)/CrimeCount)*100\n",
    "print('#### Accuracy of model:', round(100-MAPE,2), '####')\n",
    "\n",
    "# Printing month names in X-Axis\n",
    "# Printing month names in X-Axis\n",
    "MonthNames2=MonthNames+MonthNames[0:6]\n",
    "plotLabels=plt.xticks(np.arange(0,54,1),MonthNames2, rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd1938-5827-422e-956f-ab3e241befa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc9352-9a66-49eb-ab63-a033c3ac0582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3d2bc-672d-4789-b7c8-afaf5006652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "gbq_data_pyspark = spark.createDataFrame(data)\n",
    "\n",
    "# Rename columns in PySpark DataFrame\n",
    "for column in gbq_data_pyspark.columns:\n",
    "    new_column = column.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    gbq_data_pyspark = gbq_data_pyspark.withColumnRenamed(column, new_column)\n",
    "\n",
    "# Show the columns in the PySpark DataFrame\n",
    "print(gbq_data_pyspark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70eb9e-61f3-4355-91dd-5e4e3280b6a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define your BigQuery destination table\n",
    "destination_table = \"los_angeles.la_crime_data\"\n",
    "\n",
    "# Write the PySpark DataFrame to BigQuery\n",
    "gbq_data_pyspark.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", destination_table) \\\n",
    "    .option(\"project\", \"lacrimedataanalysis\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284800fa-59a5-4dd0-9ead-a9c717e39cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d1d10-775e-4b8e-b961-959f30b2f0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a707f5e1-96fc-4fbf-b32f-4da2619ddf62",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_temp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mla_crime_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m df_temp\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mla_crime_data_10000.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "df_temp = pd.read.csv(\"la_crime_data.csv\", header=True, inferSchema=True)\n",
    "df_temp.head(10000).to_csv(\"la_crime_data_10000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57e04e-2a5e-48be-bf27-4ce1f3ffef8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
